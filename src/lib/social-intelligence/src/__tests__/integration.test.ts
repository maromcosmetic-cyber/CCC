/**
 * Integration test for end-to-end ingestion pipeline
 * Verifies that all components work together correctly
 */

import { describe, test, expect, beforeEach } from '@jest/globals';
import { EventNormalizationService } from '../processing/EventNormalizationService';
import { EventDeduplicationService } from '../processing/EventDeduplicationService';
import { KafkaEventStreaming } from '../streaming/KafkaEventStreaming';
import { Platform } from '../types/core';
import { RawPlatformData } from '../ingestion/types';

describe('End-to-End Ingestion Pipeline Integration', () => {
  let normalizationService: EventNormalizationService;
  let deduplicationService: EventDeduplicationService;
  let streamingService: KafkaEventStreaming;

  beforeEach(() => {
    // Initialize services
    normalizationService = new EventNormalizationService();
    deduplicationService = new EventDeduplicationService({
      timeWindowMs: 60000,
      maxCacheSize: 1000,
      contentFields: ['text', 'mediaUrls', 'hashtags'],
      platformRules: {
        [Platform.TIKTOK]: { useContentHash: true, useTimestampWindow: true },
        [Platform.INSTAGRAM]: { useContentHash: true, useTimestampWindow: true },
        [Platform.FACEBOOK]: { useContentHash: true, useTimestampWindow: true },
        [Platform.YOUTUBE]: { useContentHash: true, useTimestampWindow: true },
        [Platform.REDDIT]: { useContentHash: true, useTimestampWindow: true },
        [Platform.RSS]: { useContentHash: true, useTimestampWindow: false }
      },
      persistence: {
        enabled: false,
        flushIntervalMs: 60000,
        retentionDays: 7
      }
    });

    // Mock Kafka client for testing
    const mockKafkaClient = {
      kafka: {
        admin: () => ({
          connect: async () => {},
          createTopics: async () => {},
          disconnect: async () => {}
        }),
        producer: () => ({
          connect: async () => {},
          send: async () => {},
          disconnect: async () => {}
        }),
        consumer: () => ({
          connect: async () => {},
          subscribe: async () => {},
          run: async () => {},
          disconnect: async () => {}
        })
      }
    };

    streamingService = new KafkaEventStreaming(mockKafkaClient as any, {
      brokers: ['localhost:9092'],
      clientId: 'test-client',
      topics: {
        rawEvents: 'raw-events-test',
        normalizedEvents: 'normalized-events-test',
        processedEvents: 'processed-events-test',
        deadLetterQueue: 'dead-letter-test',
        replayEvents: 'replay-events-test'
      },
      partitions: {
        byPlatform: true,
        byEventType: false,
        count: 3
      },
      retention: {
        rawEvents: '7d',
        normalizedEvents: '30d',
        processedEvents: '90d',
        deadLetterQueue: '30d',
        replayEvents: '7d'
      },
      replication: {
        factor: 1,
        minInSyncReplicas: 1
      }
    });
  });

  test('should process raw data through complete pipeline', async () => {
    // Sample raw data from different platforms
    const sampleRawData: RawPlatformData[] = [
      {
        id: 'tiktok-123',
        platform: Platform.TIKTOK,
        timestamp: new Date().toISOString(),
        type: 'post',
        data: {
          text: 'Amazing hair transformation! #haircare #beauty',
          author_id: 'user123',
          username: 'beautyinfluencer',
          display_name: 'Beauty Influencer',
          follower_count: 50000,
          likes: 1200,
          shares: 45,
          comments: 89,
          views: 15000
        },
        metadata: {
          source: 'api',
          version: '1.0'
        }
      },
      {
        id: 'instagram-456',
        platform: Platform.INSTAGRAM,
        timestamp: new Date().toISOString(),
        type: 'post',
        data: {
          message: 'Love this new hair serum! Results are incredible ðŸ’«',
          from: {
            id: 'user456',
            name: 'Hair Enthusiast'
          },
          likes: { summary: { total_count: 890 } },
          comments: { summary: { total_count: 67 } },
          shares: { count: 23 }
        },
        metadata: {
          source: 'webhook',
          version: '1.0'
        }
      }
    ];

    // Step 1: Process through normalization
    const normalizedEvents = [];
    for (const rawData of sampleRawData) {
      const normalized = await normalizationService.normalize(rawData);
      normalizedEvents.push(normalized);
    }

    // Verify normalization worked
    expect(normalizedEvents).toHaveLength(2);
    expect(normalizedEvents[0].platform).toBe(Platform.TIKTOK);
    expect(normalizedEvents[1].platform).toBe(Platform.INSTAGRAM);
    expect(normalizedEvents[0].content.text).toContain('hair transformation');
    expect(normalizedEvents[1].content.text).toContain('hair serum');

    // Step 2: Process through deduplication
    const deduplicationResults = [];
    for (const rawData of sampleRawData) {
      const result = await deduplicationService.processEvent(rawData);
      deduplicationResults.push(result);
    }

    // Verify deduplication worked
    expect(deduplicationResults).toHaveLength(2);
    expect(deduplicationResults[0].isDuplicate).toBe(false);
    expect(deduplicationResults[1].isDuplicate).toBe(false);
    expect(deduplicationResults[0].uniqueId).toBeDefined();
    expect(deduplicationResults[1].uniqueId).toBeDefined();
    expect(deduplicationResults[0].uniqueId).not.toBe(deduplicationResults[1].uniqueId);

    // Step 3: Verify event streaming is properly configured (skip for mock)
    // const topics = await streamingService.getTopics();
    // expect(Array.isArray(topics)).toBe(true);

    // Step 4: Verify metrics are being tracked
    const normalizationMetrics = normalizationService.getMetrics();
    expect(normalizationMetrics.totalProcessed).toBe(2);
    expect(normalizationMetrics.successfulNormalizations).toBe(2);
    expect(normalizationMetrics.failedNormalizations).toBe(0);

    const deduplicationMetrics = deduplicationService.getMetrics();
    expect(deduplicationMetrics.totalProcessed).toBe(2);
    expect(deduplicationMetrics.uniqueEvents).toBe(2);
    expect(deduplicationMetrics.duplicatesDetected).toBe(0);
  });

  test('should handle duplicate detection correctly', async () => {
    const originalData: RawPlatformData = {
      id: 'duplicate-test-123',
      platform: Platform.REDDIT,
      timestamp: new Date().toISOString(),
      type: 'post',
      data: {
        title: 'Best hair care routine for damaged hair',
        selftext: 'Looking for recommendations...',
        author: 'haircare_seeker',
        ups: 45,
        downs: 2,
        num_comments: 12
      },
      metadata: {
        source: 'api',
        version: '1.0'
      }
    };

    // Process original event
    const firstResult = await deduplicationService.processEvent(originalData);
    expect(firstResult.isDuplicate).toBe(false);
    expect(firstResult.uniqueId).toBeDefined();

    // Process same event again (should be detected as duplicate)
    const secondResult = await deduplicationService.processEvent(originalData);
    expect(secondResult.isDuplicate).toBe(true);
    expect(secondResult.duplicateOf).toBe(firstResult.uniqueId);
    expect(secondResult.confidence).toBeGreaterThan(0.9);
  });

  test('should handle normalization errors gracefully', async () => {
    const invalidData: RawPlatformData = {
      id: 'invalid-123',
      platform: Platform.YOUTUBE,
      timestamp: 'invalid-timestamp', // Invalid timestamp
      type: 'post',
      data: {
        // Missing required fields
      },
      metadata: {
        source: 'api',
        version: '1.0'
      }
    };

    // Should handle invalid data gracefully
    try {
      await normalizationService.normalize(invalidData);
      // If it doesn't throw, check that it handled the error gracefully
      const metrics = normalizationService.getMetrics();
      expect(metrics.totalProcessed).toBeGreaterThan(0);
    } catch (error) {
      // If it throws, that's also acceptable for invalid data
      expect(error).toBeDefined();
    }
  });

  test('should maintain data lineage through pipeline', async () => {
    const testData: RawPlatformData = {
      id: 'lineage-test-789',
      platform: Platform.RSS,
      timestamp: new Date().toISOString(),
      type: 'post',
      data: {
        title: 'Revolutionary Hair Care Discovery',
        content: 'Scientists discover new ingredient...',
        author: 'Science News',
        link: 'https://example.com/article'
      },
      metadata: {
        source: 'crawler',
        version: '1.0'
      }
    };

    // Process through normalization
    const normalized = await normalizationService.normalize(testData);

    // Verify lineage is maintained
    expect(normalized.platform).toBe(testData.platform);
    expect(normalized.platformId).toBe(testData.id);
    expect(normalized.metadata.source).toBe(testData.metadata?.source);
    expect(normalized.metadata.processingTimestamp).toBeDefined();
    expect(normalized.metadata.version).toBeDefined();

    // Process through deduplication
    const deduplicationResult = await deduplicationService.processEvent(testData);

    // Verify unique ID is generated and traceable
    expect(deduplicationResult.uniqueId).toBeDefined();
    expect(deduplicationResult.uniqueId).toContain(testData.platform.toUpperCase().substring(0, 3));
  });
});